{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lưu ý\n",
    "- trong spark streaming dùng readStream để xử lý dữ liệu\n",
    "- và dùng writeStreaming để ghi dữ liệu ra màn hình hoặc lưu dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('kafka-streaming')\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Khởi tạo Kafka consumer\n",
    "kafka_bootstrap_servers = 'kafka1:19091,kafka2:19092,kafka3:19093' # Địa chỉ Kafka của các container Kafka\n",
    "kafka_topic = 'USDCHF_i30'  # Tên topic Kafka\n",
    "\n",
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 0 is empty, skipping.\n",
      "Batch 1 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 2 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 3 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 4 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 5 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 6 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 7 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 8 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 9 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 10 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 11 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 12 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 13 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 14 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Error sending batch 15: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 16: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 17: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 18: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 19: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 20: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 21: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 22: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 23: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 24: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 25: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 26: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 27: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 28: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 29: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 30: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 31: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 32: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 33: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Error sending batch 34: 422 Client Error: Unprocessable Entity for url: http://host.docker.internal:8000/api/sparkreceive/predictions_values\n",
      "Batch 35 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 36 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 37 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 38 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 39 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 40 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 41 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 42 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 43 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 44 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 45 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 46 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 47 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 48 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 49 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 50 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n",
      "Batch 51 successfully sent. Response: {\"message\":\"JSON parsed and received successfully\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, explode, arrays_zip, lag, when, avg, pow, sqrt, mean, stddev\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "# Tạo SparkSession\n",
    "spark = SparkSession.builder.appName(\"jsonStreaming\").getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "jsonSchema = StructType([\n",
    "    StructField(\"date\", ArrayType(StringType())),\n",
    "    StructField(\"open\", ArrayType(DoubleType())),\n",
    "    StructField(\"high\", ArrayType(DoubleType())),\n",
    "    StructField(\"low\", ArrayType(DoubleType())),\n",
    "    StructField(\"close\", ArrayType(DoubleType())),\n",
    "    StructField(\"volume\", ArrayType(IntegerType()))\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ nguồn (ví dụ: Kafka, file, ...)\n",
    "\n",
    "# Hàm tính EMA\n",
    "def calculate_ema(column, period):\n",
    "    alpha = 1 - (2 / (period + 1))  # Smoothing factor for EMA\n",
    "    window = Window.orderBy('Date').rowsBetween(Window.unboundedPreceding, 0)\n",
    "    ema = avg(column).over(window)\n",
    "    return ema\n",
    "\n",
    "# Hàm tính RSI\n",
    "def calculate_rsi(df, column, period):\n",
    "    window = Window.orderBy('Date')\n",
    "    daily_change = col(column) - lag(column, 1).over(window)\n",
    "    gain = when(daily_change > 0, daily_change).otherwise(0)\n",
    "    loss = when(daily_change < 0, -daily_change).otherwise(0)\n",
    "    avg_gain = avg(gain).over(Window.orderBy('Date').rowsBetween(-period, -1))\n",
    "    avg_loss = avg(loss).over(Window.orderBy('Date').rowsBetween(-period, -1))\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Hàm tính Turbulence\n",
    "def calculate_turbulence(df, window_period):\n",
    "    windowSpec = Window.orderBy('Date').rowsBetween(-window_period, -1)\n",
    "    mean_close = mean('Close').over(windowSpec)\n",
    "    stddev_close = stddev('Close').over(windowSpec)\n",
    "    turbulence = (col('Close') - mean_close) / stddev_close\n",
    "    return df.withColumn('Turbulence', turbulence)\n",
    "import requests\n",
    "\n",
    "# Hàm xử lý từng batch\n",
    "import requests\n",
    "import json\n",
    "def write_to_pandas(df, epoch_id):\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"Batch {epoch_id} is empty, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Existing processing code\n",
    "    df = df.withColumn('EMA', calculate_ema('Close', 12))\n",
    "    df = df.withColumn('RSI', calculate_rsi(df, 'Close', 14))\n",
    "    df = calculate_turbulence(df, 20)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    # Handle NaN values for JSON conversion\n",
    "    pandas_df = pandas_df.where(pd.notnull(pandas_df), None)\n",
    "\n",
    "    # Convert the DataFrame to a JSON string\n",
    "    json_payload = json.dumps(pandas_df.to_json(orient='records'))\n",
    "#     print(json_payload)\n",
    "    # Define the URL of your FastAPI endpoint\n",
    "    url = 'http://host.docker.internal:8000/api/sparkreceive/predictions_values'\n",
    "\n",
    "    # Make a POST request to the FastAPI endpoint\n",
    "    try:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json_payload, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        print(f\"Batch {epoch_id} successfully sent. Response: {response.text}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error sending batch {epoch_id}: {e}\")\n",
    "\n",
    "# Rest of your Spark streaming code\n",
    "\n",
    "    \n",
    "# Chuyển đổi và phân tích cú pháp JSON\n",
    "df_parsed = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "              .select(from_json(col(\"value\"), jsonSchema).alias(\"data\")) \\\n",
    "              .select(\"data.*\")\n",
    "\n",
    "# Kết hợp các mảng thành dòng riêng biệt\n",
    "df_exploded = df_parsed.select(\n",
    "    explode(arrays_zip(\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\")).alias(\"data\")\n",
    ").select(\n",
    "    col(\"data.date\").alias(\"Date\"),\n",
    "    col(\"data.open\").alias(\"Open\"),\n",
    "    col(\"data.high\").alias(\"High\"),\n",
    "    col(\"data.low\").alias(\"Low\"),\n",
    "    col(\"data.close\").alias(\"Close\"),\n",
    "    col(\"data.volume\").alias(\"Volume\")\n",
    ")\n",
    "\n",
    "# Áp dụng hàm xử lý cho từng batch\n",
    "query = df_exploded.writeStream \\\n",
    "                   .outputMode(\"append\") \\\n",
    "                   .foreachBatch(write_to_pandas) \\\n",
    "                   .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error sending request: HTTPConnectionPool(host='host.docker.internal', port=8000): Max retries exceeded with url: /api/predictions_values (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7feb7a190d90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL of your FastAPI endpoint\n",
    "url = 'http://host.docker.internal:8000/api/predictions_values'\n",
    "\n",
    "# Sample JSON payload as a string\n",
    "sample_payload = json.dumps([\n",
    "    {\n",
    "        \"Date\": \"2023-01-01\",\n",
    "        \"Open\": 100.5,\n",
    "        \"High\": 110.2,\n",
    "        \"Low\": 99.8,\n",
    "        \"Close\": 105.3,\n",
    "        \"Volume\": 5000,\n",
    "        \"EMA\": 104.7,\n",
    "        \"RSI\": 60.5,\n",
    "        \"Turbulence\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"Date\": \"2023-01-02\",\n",
    "        \"Open\": 105.4,\n",
    "        \"High\": 115.2,\n",
    "        \"Low\": 104.8,\n",
    "        \"Close\": 110.3,\n",
    "        \"Volume\": 6000,\n",
    "        \"EMA\": 109.7,\n",
    "        \"RSI\": 65.5,\n",
    "        \"Turbulence\": 0.4\n",
    "    }\n",
    "])\n",
    "\n",
    "\n",
    "# Make a POST request to the FastAPI endpoint\n",
    "try:\n",
    "    headers = {'Content-Type': 'text/plain'}\n",
    "    response = requests.post(url, data=sample_payload, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    print(f\"Request successfully sent. Response: {response.text}\")\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error sending request: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HTTPConnectionPool(host='host.docker.internal', port=8000): Max retries exceeded with url: /api/models/?skip=0&limit=10 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7feb93473ad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL for the FastAPI endpoint\n",
    "url = \"http://host.docker.internal:8000/api/models/\"\n",
    "\n",
    "# Set the parameters for the request\n",
    "params = {\n",
    "    \"skip\": 0,\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "# Attempt to send a GET request to the FastAPI endpoint\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        # If the request is successful, print the response content\n",
    "        result = response.json()\n",
    "    else:\n",
    "        # If the request fails, print the status code\n",
    "        result = f\"Request failed with status code: {response.status_code}\"\n",
    "except requests.RequestException as e:\n",
    "    # If there is an error in sending the request, print the error\n",
    "    result = str(e)\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
